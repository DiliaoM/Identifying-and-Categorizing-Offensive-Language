{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/olidv10/olid-training-v1.0.tsv\n",
      "/kaggle/input/olidv10/testset-levelb.tsv\n",
      "/kaggle/input/olidv10/labels-levelb.csv\n",
      "/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from gensim.models import FastText\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras import utils\n",
    "from keras.models import Model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dropout, Conv1D, MaxPooling1D, LSTM,Concatenate, Dense, GlobalMaxPooling1D,GlobalAveragePooling1D, Lambda, Input, Bidirectional, GRU, concatenate, SpatialDropout1D, Reshape, merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from keras.preprocessing import text,sequence\n",
    "\n",
    "#import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "5  97670                  @USER Liberals are all Kookoo !!!       OFF   \n",
       "6  77444                   @USER @USER Oh noes! Tough shit.       OFF   \n",
       "7  52415  @USER was literally just talking about this lo...       OFF   \n",
       "8  45157                         @USER Buy more icecream!!!       NOT   \n",
       "9  13384  @USER Canada doesn’t need another CUCK! We alr...       OFF   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT      NULL  \n",
       "1       TIN       IND  \n",
       "2      NULL      NULL  \n",
       "3       UNT      NULL  \n",
       "4      NULL      NULL  \n",
       "5       TIN       OTH  \n",
       "6       UNT      NULL  \n",
       "7       TIN       GRP  \n",
       "8      NULL      NULL  \n",
       "9       TIN       IND  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../input/olidv10/olid-training-v1.0.tsv', sep = '\\t',engine='python',encoding = 'utf-8-sig')\n",
    "\n",
    "data = data.fillna('NULL')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 13240\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13240 entries, 0 to 13239\n",
      "Data columns (total 5 columns):\n",
      "id           13240 non-null int64\n",
      "tweet        13240 non-null object\n",
      "subtask_a    13240 non-null object\n",
      "subtask_b    13240 non-null object\n",
      "subtask_c    13240 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 517.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size:',len(data))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing:\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stops = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def script_preprocessing(df):\n",
    "    \n",
    "    def removewords(text): # Remove these words.\n",
    "        \n",
    "        text = text.replace('@USER','',50)\n",
    "        text = text.strip('URL')\n",
    "        text = text.replace('&amp','',10)\n",
    "        return text\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def pos_lemma(text):\n",
    "        \n",
    "        tokens = [i for i in tokenizer.tokenize(str(removewords(text)).lower()) if i not in stops] # if we don't convert text to str, it rises TypeError: expected string or bytes-like object   \n",
    "        tagged = pos_tag(tokens)\n",
    "        lemlist = [lemmatizer.lemmatize(i[0], get_wordnet_pos(i[1])) for i in tagged]\n",
    "        lemmas = ' '.join(lemlist).lower()\n",
    "\n",
    "        return lemmas\n",
    "    \n",
    "    def countuser(text):\n",
    "        splitted_text = text.lower().split()\n",
    "        user_count = 0\n",
    "        for word in splitted_text:\n",
    "            word = re.sub(\"[#@]\",\"\",word)\n",
    "            word = re.sub(\"!\",\" !\",word)\n",
    "            word = re.sub(\"[?]\",\" ?\",word)\n",
    "            if(word == 'user'):\n",
    "                user_count += 1\n",
    "        return user_count\n",
    "                        \n",
    "    def finalize(df):\n",
    "        \n",
    "        df['pos_lemmatized'] = [pos_lemma(i) for i in df['tweet']]\n",
    "        df['user_count'] = [countuser(i) for i in df['tweet']]\n",
    "        return df\n",
    "        \n",
    "    return finalize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>pos_lemmatized</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ask native american take</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>go home drunk maga trump2020</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>someone vetaken piece shit volcano</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>obama want liberal illegals move red state</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c                                     pos_lemmatized  \\\n",
       "0       UNT      NULL                           ask native american take   \n",
       "1       TIN       IND                       go home drunk maga trump2020   \n",
       "2      NULL      NULL  amazon investigate chinese employee sell inter...   \n",
       "3       UNT      NULL                 someone vetaken piece shit volcano   \n",
       "4      NULL      NULL         obama want liberal illegals move red state   \n",
       "\n",
       "   user_count  \n",
       "0           1  \n",
       "1           3  \n",
       "2           0  \n",
       "3           1  \n",
       "4           2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = script_preprocessing(data)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>pos_lemmatized</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ask native american take</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>go home drunk maga trump2020</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>someone vetaken piece shit volcano</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>obama want liberal illegals move red state</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c                                     pos_lemmatized  \\\n",
       "0       UNT      NULL                           ask native american take   \n",
       "1       TIN       IND                       go home drunk maga trump2020   \n",
       "2      NULL      NULL  amazon investigate chinese employee sell inter...   \n",
       "3       UNT      NULL                 someone vetaken piece shit volcano   \n",
       "4      NULL      NULL         obama want liberal illegals move red state   \n",
       "\n",
       "   user_count  \n",
       "0        0.02  \n",
       "1        0.06  \n",
       "2        0.00  \n",
       "3        0.02  \n",
       "4        0.04  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ = result['user_count'].max()\n",
    "result['user_count'] = result['user_count'].apply(lambda x:x/max_)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>pos_lemmatized</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ask native american take</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>go home drunk maga trump2020</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>someone vetaken piece shit volcano</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>liberal kookoo</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>oh no tough shit</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "5  97670                  @USER Liberals are all Kookoo !!!       OFF   \n",
       "6  77444                   @USER @USER Oh noes! Tough shit.       OFF   \n",
       "\n",
       "  subtask_b subtask_c                      pos_lemmatized  user_count  \n",
       "0       UNT      NULL            ask native american take        0.02  \n",
       "1       TIN       IND        go home drunk maga trump2020        0.06  \n",
       "3       UNT      NULL  someone vetaken piece shit volcano        0.02  \n",
       "5       TIN       OTH                      liberal kookoo        0.02  \n",
       "6       UNT      NULL                    oh no tough shit        0.04  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the valid data for subtask2\n",
    "ind = []\n",
    "for i in range(len(result['subtask_b'])):\n",
    "    if result['subtask_b'][i] == 'NULL':\n",
    "        ind.append(i)\n",
    "result.drop(axis=0,labels = ind,inplace = True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_b = result[result[\"subtask_b\"].notna()]\n",
    "X = result[['pos_lemmatized',\"user_count\"]]\n",
    "y = result['subtask_b']\n",
    "label_to_number = {'UNT':0,'TIN':1}\n",
    "number_to_label = {v:k for k,v in label_to_number.items()}\n",
    "Y = result.subtask_b.apply(lambda x:label_to_number[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine oversampling and undersampling in order to have balanced dataset\n",
    "def same_size_data(X_train, count_train, y_train, ratio_down_over_up=0.5):\n",
    "    X_train = list(X_train)\n",
    "    count_train = list(count_train)\n",
    "    y_train = list(y_train)\n",
    "  \n",
    "    n_cat = len(Counter(y_train))\n",
    "  \n",
    "    sorted_counter = Counter(y_train).most_common()\n",
    "    max_cat = sorted_counter[0][1]\n",
    "    min_cat = sorted_counter[-1][1]\n",
    "\n",
    "    target = min_cat + (1-ratio_down_over_up)*(max_cat - min_cat)\n",
    "\n",
    "    for i in range(n_cat):\n",
    "        diff = int(sorted_counter[i][1] - target)\n",
    "        k = 0\n",
    "        if diff > 0:\n",
    "            rm = 0    \n",
    "            while rm <= diff:\n",
    "                if(y_train[k] == sorted_counter[i][0]):\n",
    "                    X_train.pop(k)\n",
    "                    y_train.pop(k)\n",
    "                    count_train.pop(k)\n",
    "                    rm += 1\n",
    "                    k -=1\n",
    "                k += 1\n",
    "        else:\n",
    "            ad = 0\n",
    "            while ad <= -diff:\n",
    "                if(y_train[k] == sorted_counter[i][0]):\n",
    "                    X_train.append(X_train[k])\n",
    "                    y_train.append(y_train[k])\n",
    "                    count_train.append(count_train[k])\n",
    "                    ad += 1\n",
    "                k += 1\n",
    "\n",
    "    return X_train, count_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b, count_train_b, y_train_b = same_size_data(X.pos_lemmatized, X.user_count, Y, 0.2)\n",
    "X_train_b, count_train_b, y_train_b = shuffle(X_train_b, count_train_b, y_train_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liberal', 'gun', 'like', 'control', 'get', 'people', 'go', 'shit', 'antifa', 'say', 'fuck', 'maga', 'know', 'conservative', 'think', 'trump', 'one', 'make', 'u', 'want', 'right', 'need', 'good', 'woman', 'would', 'democrat', 'lie', 'see', 'time', 'take']\n"
     ]
    }
   ],
   "source": [
    "# Bag of words model.\n",
    "all_words_list = []\n",
    "for sent in data['pos_lemmatized']:\n",
    "    for word in sent.split(' '):\n",
    "        all_words_list.append(word)\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(5000)\n",
    "\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:30])\n",
    "\n",
    "#word_features = tokenizer.texts_to_sequences(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDFmatrix(X_train,word_features):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    fit = vectorizer.fit(word_features)\n",
    "    X = vectorizer.transform(X_train)\n",
    "    X = X.todense()\n",
    "    return X\n",
    "    \n",
    "X_train = TFIDFmatrix(X_train_b,word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X,y,clf):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state = 52)\n",
    "    iteration_index = 0\n",
    "    acc_list_scores = []\n",
    "    f1_list_scores = []\n",
    "    for train_indexes, test_indexes in kf.split(X, y):\n",
    "        iteration_index += 1\n",
    "\n",
    "        X_train = X[train_indexes]\n",
    "        y_train = y[train_indexes]\n",
    "\n",
    "        X_test = X[test_indexes]\n",
    "        y_test = y[test_indexes]\n",
    "\n",
    "        #logreg = linear_model.LogisticRegression(C=1e5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict = clf.predict(X_test)\n",
    "\n",
    "        current_acc = accuracy_score(y_test, y_predict)\n",
    "        current_f1 = f1_score(y_test, y_predict, average = 'macro')\n",
    "        print(\"Iteration #{0}: Accuracy : {1}, F-score : {2}\".format(iteration_index, current_acc, current_f1))\n",
    "        #print(classification_report(y_test, y_predict, target_names= [\"UNT\",'TIN']))\n",
    "        acc_list_scores.append(current_acc)\n",
    "        f1_list_scores.append(current_f1)\n",
    "        \n",
    "\n",
    "    print(\"Accuracy: {0}\".format(np.mean(acc_list_scores)))\n",
    "    print(\"F1-measure: {0}\".format(np.mean(f1_list_scores)))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.8986749805144193, F-score : 0.897213252150763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #2: Accuracy : 0.8931357254290172, F-score : 0.8922069544969415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #3: Accuracy : 0.890015600624025, F-score : 0.8886688761014583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #4: Accuracy : 0.8853354134165367, F-score : 0.8846475198059891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #5: Accuracy : 0.9017160686427457, F-score : 0.900277069874948\n",
      "Accuracy: 0.8937755577253487\n",
      "F1-measure: 0.89260273448602\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "LRmodel = training(X_train,np.array(y_train_b),logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.8612626656274357, F-score : 0.8577718709986\n",
      "Iteration #2: Accuracy : 0.8728549141965679, F-score : 0.8710661846595624\n",
      "Iteration #3: Accuracy : 0.8806552262090483, F-score : 0.8789308166395595\n",
      "Iteration #4: Accuracy : 0.8822152886115444, F-score : 0.8812930207173977\n",
      "Iteration #5: Accuracy : 0.8806552262090483, F-score : 0.878227895723952\n",
      "Accuracy: 0.8755286641707289\n",
      "F1-measure: 0.8734579577478143\n"
     ]
    }
   ],
   "source": [
    "# NaiveBayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "NBmodel = training(X_train,np.array(y_train_b),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.936087295401403, F-score : 0.9356563412808305\n",
      "Iteration #2: Accuracy : 0.9305772230889235, F-score : 0.9303258742240907\n",
      "Iteration #3: Accuracy : 0.9266770670826833, F-score : 0.9262807336935244\n",
      "Iteration #4: Accuracy : 0.9407176287051482, F-score : 0.940665497417909\n",
      "Iteration #5: Accuracy : 0.9399375975039002, F-score : 0.9395319690854328\n",
      "Accuracy: 0.9347993623564117\n",
      "F1-measure: 0.9344920831403576\n"
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=30)\n",
    "RFmodel=training(X_train,np.array(y_train_b),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.8160561184723305, F-score : 0.81240458771982\n",
      "Iteration #2: Accuracy : 0.8198127925117005, F-score : 0.8172778445175394\n",
      "Iteration #3: Accuracy : 0.8057722308892356, F-score : 0.8020843214609222\n",
      "Iteration #4: Accuracy : 0.8081123244929798, F-score : 0.8059927360774818\n",
      "Iteration #5: Accuracy : 0.7987519500780031, F-score : 0.7945191182281066\n",
      "Accuracy: 0.8097010832888498\n",
      "F1-measure: 0.8064557216007741\n"
     ]
    }
   ],
   "source": [
    "# GradientBoosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "GBmodel = training(X_train,np.array(y_train_b),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function.\n",
    "def testify(x_test,y_test,model):\n",
    "    y_predict = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test,y_predict)\n",
    "    #f1 = f1_score(y_test, y_predict, average = 'macro')\n",
    "    print(classification_report(y_test, y_predict, target_names= [\"UNT\",'TIN']))\n",
    "    print('acc:',acc)\n",
    "    #print('f1:',f1)\n",
    "    #return acc,f1\n",
    "\n",
    "\n",
    "# Import Test_data.\n",
    "testdata = pd.read_csv('../input/olidv10/testset-levelb.tsv', sep = '\\t',engine='python',encoding = 'utf-8-sig')\n",
    "testdata = testdata.fillna('NULL')\n",
    "testdata = script_preprocessing(testdata)\n",
    "x_test = TFIDFmatrix(testdata['pos_lemmatized'],word_features)\n",
    "y_test = pd.read_csv('../input/olidv10/labels-levelb.csv',sep = ',',engine='python',header = None,encoding = 'utf-8-sig')\n",
    "y_test = y_test[1].apply(lambda x:label_to_number[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.20      0.37      0.26        27\n",
      "         TIN       0.91      0.81      0.86       213\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.56      0.59      0.56       240\n",
      "weighted avg       0.83      0.76      0.79       240\n",
      "\n",
      "acc: 0.7625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.22      0.48      0.30        27\n",
      "         TIN       0.92      0.78      0.84       213\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.57      0.63      0.57       240\n",
      "weighted avg       0.84      0.75      0.78       240\n",
      "\n",
      "acc: 0.7458333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.26      0.81      0.40        27\n",
      "         TIN       0.97      0.71      0.82       213\n",
      "\n",
      "    accuracy                           0.72       240\n",
      "   macro avg       0.61      0.76      0.61       240\n",
      "weighted avg       0.89      0.72      0.77       240\n",
      "\n",
      "acc: 0.7208333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.44      0.56      0.49        27\n",
      "         TIN       0.94      0.91      0.93       213\n",
      "\n",
      "    accuracy                           0.87       240\n",
      "   macro avg       0.69      0.73      0.71       240\n",
      "weighted avg       0.89      0.87      0.88       240\n",
      "\n",
      "acc: 0.8708333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model testing:\n",
    "testify(x_test,y_test,LRmodel)\n",
    "testify(x_test,y_test,NBmodel)\n",
    "testify(x_test,y_test,GBmodel)\n",
    "testify(x_test,y_test,RFmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create word embedding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embedding.\n",
    "\n",
    "maxlen = 10\n",
    "# Tokenize each word in the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_b)\n",
    "Xtr_seq = tokenizer.texts_to_sequences(X_train_b)\n",
    "Xtr_seq = sequence.pad_sequences(Xtr_seq,maxlen=maxlen)\n",
    "Xte_seq = tokenizer.texts_to_sequences(testdata['pos_lemmatized'])\n",
    "Xte_seq = sequence.pad_sequences(Xte_seq,maxlen=maxlen)\n",
    "preprocessed = tokenizer.sequences_to_texts(Xtr_seq)\n",
    "sentences = [t.split() for t in preprocessed]\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix(using glove method)\n",
    "EMBEDDING_FILE = '../input/glove6b100dtxt/glove.6B.100d.txt'\n",
    "Embedding_size = 100\n",
    "threshold = 0.4\n",
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word,np.asarray(arr,dtype='float32')\n",
    "\n",
    "Embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "max_features = len(word_index)+1\n",
    "Embedding_matrix = np.zeros((max_features,Embedding_size))\n",
    "for word,i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = Embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            Embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        print(word)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 100)      775200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 10, 100)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 10, 256)      234496      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 9, 64)        32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1290        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,043,829\n",
      "Trainable params: 1,043,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "inp = Input(shape = (maxlen,))\n",
    "\n",
    "x = Embedding(max_features, Embedding_size, weights = [Embedding_matrix], trainable = True)(inp)\n",
    "x = SpatialDropout1D(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
    "x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool])\n",
    "x = Dense(10, activation = \"relu\")(x)\n",
    "\n",
    "a = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model_b = Model(inputs = inp, outputs = a)\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5128 samples, validate on 1283 samples\n",
      "Epoch 1/4\n",
      "5128/5128 [==============================] - 9s 2ms/step - loss: 0.6554 - accuracy: 0.6092 - val_loss: 0.6489 - val_accuracy: 0.6220\n",
      "Epoch 2/4\n",
      "5128/5128 [==============================] - 5s 990us/step - loss: 0.6016 - accuracy: 0.6722 - val_loss: 0.5660 - val_accuracy: 0.6867\n",
      "Epoch 3/4\n",
      "5128/5128 [==============================] - 5s 991us/step - loss: 0.5185 - accuracy: 0.7440 - val_loss: 0.4515 - val_accuracy: 0.7857\n",
      "Epoch 4/4\n",
      "5128/5128 [==============================] - 5s 1ms/step - loss: 0.4194 - accuracy: 0.8153 - val_loss: 0.3520 - val_accuracy: 0.8223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNT       0.28      0.41      0.33        27\n",
      "         TIN       0.92      0.86      0.89       213\n",
      "\n",
      "    accuracy                           0.81       240\n",
      "   macro avg       0.60      0.64      0.61       240\n",
      "weighted avg       0.85      0.81      0.83       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_b.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(Xtr_seq, y_train_b, train_size=0.8,random_state=233)\n",
    "#model_b.fit(np.array(X_train_b), y_train_b, validation_data=(np.array(testdata['pos_lemmatized']),y_test), epochs = 3)\n",
    "model_b.fit(np.array(X_tra), y_tra, validation_data=(X_val,y_val), epochs = 4)\n",
    "\n",
    "y_pred_b = model_b.predict(Xte_seq)\n",
    "y_pred_b = y_pred_b > threshold\n",
    "print(classification_report(y_test, y_pred_b, target_names= [\"UNT\",'TIN']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
